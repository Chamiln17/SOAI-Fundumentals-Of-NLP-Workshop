{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from glove import Corpus, Glove\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extended dataset\n",
    "dataset = [\n",
    "    \"word embeddings are fascinating\",\n",
    "    \"word2vec captures semantic relationships\",\n",
    "    \"GloVe considers global context\",\n",
    "    \"FastText extends Word2Vec with subword information\",\n",
    "    \"natural language processing involves understanding text\",\n",
    "    \"machine learning powers NLP applications\",\n",
    "    \"text embeddings help in semantic similarity\",\n",
    "    \"skip-gram is a technique in word2vec\",\n",
    "    \"CBOW focuses on context prediction\",\n",
    "    \"neural networks model language effectively\",\n",
    "    \"GloVe captures co-occurrence statistics\",\n",
    "    \"FastText models subword units\",\n",
    "    \"contextual embeddings enhance NLP models\",\n",
    "    \"transformers have revolutionized NLP tasks\",\n",
    "    \"attention mechanisms improve model focus\",\n",
    "    \"sequence-to-sequence models handle translation tasks\",\n",
    "    \"large language models power generative AI\",\n",
    "    \"embedding spaces visualize semantic similarity\"\n",
    "]\n",
    "\n",
    "# Utility function for preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Tokenize sentences into word lists.\"\"\"\n",
    "    return [sentence.split() for sentence in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to train Word2Vec model\n",
    "def train_word2vec(data):\n",
    "    tokenized_data = preprocess_data(data)\n",
    "    model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Function to train GloVe model\n",
    "def train_glove(data):\n",
    "    tokenized_data = preprocess_data(data)\n",
    "    corpus = Corpus()\n",
    "    corpus.fit(tokenized_data, window=5)\n",
    "    glove = Glove(no_components=100, learning_rate=0.05)\n",
    "    glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "    glove.add_dictionary(corpus.dictionary)\n",
    "    return glove\n",
    "\n",
    "# Function to train FastText model\n",
    "def train_fasttext(data):\n",
    "    tokenized_data = preprocess_data(data)\n",
    "    model = FastText(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot embeddings\n",
    "def plot_embeddings(model, title):\n",
    "    \"\"\"\n",
    "    Plot 2D representations of word embeddings using t-SNE.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle GloVe-specific structure\n",
    "        if isinstance(model, Glove):\n",
    "            labels = list(model.dictionary.keys())\n",
    "            vectors = [model.word_vectors[model.dictionary[word]] for word in labels]\n",
    "        else:\n",
    "            labels = model.wv.index_to_key\n",
    "            vectors = [model.wv[word] for word in labels]\n",
    "        \n",
    "        # Reduce dimensions with t-SNE\n",
    "        tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "        new_values = tsne_model.fit_transform(vectors)\n",
    "        \n",
    "        # Plotting\n",
    "        x, y = zip(*new_values)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.scatter(x, y)\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.annotate(label, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during plotting: {e}\")\n",
    "\n",
    "# Train models\n",
    "word2vec_model = train_word2vec(dataset)\n",
    "glove_model = train_glove(dataset)\n",
    "fasttext_model = train_fasttext(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot embeddings\n",
    "plot_embeddings(word2vec_model, 'Word2Vec Embeddings')\n",
    "plot_embeddings(glove_model, 'GloVe Embeddings')\n",
    "plot_embeddings(fasttext_model, 'FastText Embeddings')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
