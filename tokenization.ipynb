{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numpy scipy scikit-learn nltk spacy matplotlib gensim\n",
    "\n",
    "# If glove-python is required, install it (may fail on newer Python versions)\n",
    "!pip install git+https://github.com/maciejkula/glove-python.git\n",
    "\n",
    "# Download GloVe pre-trained embeddings (e.g., 100-dimensional vectors)\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip -d glove.6B\n",
    "\n",
    "# Download NLTK data (if required)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set up SpaCy (if needed)\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Artificial intelligence (AI) is revolutionizing industries worldwide. Companies like OpenAI and Google are at the forefront, \n",
    "developing large language models such as ChatGPT and Bard. In 2024, advancements in AI are expected to enhance medical diagnoses, \n",
    "autonomous driving, and personalized education.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Preprocessing with NLTK\n",
    "def nltk_preprocessing(text):\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"Sentences:\", sentences)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(text)\n",
    "    print(\"\\nWords:\", words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    print(\"\\nFiltered Words:\", filtered_words)\n",
    "\n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Word Frequency Distribution with NLTK\n",
    "def nltk_word_frequency(words):\n",
    "    freq_dist = FreqDist(words)\n",
    "    print(\"\\nWord Frequency:\", freq_dist.most_common(5))\n",
    "\n",
    "    # Visualize word frequency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    freq_dist.plot(10, cumulative=False)\n",
    "    plt.title(\"Word Frequency Distribution\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Entity Recognition and Visualization with spaCy\n",
    "def spacy_ner_visualization(text):\n",
    "    # Load spaCy's English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Display entities\n",
    "    print(\"\\nNamed Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "    # Visualize entities using spaCy's DisplaCy\n",
    "    displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Part-of-Speech Tagging with spaCy\n",
    "def spacy_pos_analysis(text):\n",
    "    # Load spaCy's English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # POS tagging\n",
    "    print(\"\\nPart-of-Speech Tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
    "\n",
    "    # Visualize POS tagging (e.g., dependency parse tree)\n",
    "    displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "print(\"\\n--- NLTK Preprocessing ---\")\n",
    "filtered_words = nltk_preprocessing(text)\n",
    "nltk_word_frequency(filtered_words)\n",
    "\n",
    "print(\"\\n--- spaCy NER Visualization ---\")\n",
    "spacy_ner_visualization(text)\n",
    "\n",
    "print(\"\\n--- spaCy POS Analysis ---\")\n",
    "spacy_pos_analysis(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
